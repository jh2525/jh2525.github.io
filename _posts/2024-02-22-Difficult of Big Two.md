---
title:  "3.빅투 학습하기 - 빅투의 어려움"
excerpt: "2.Reinforcement learning in Big Two - difficult of Big Two"

categories:
  - BigTwo
tags:
  - [BigTwo, RL]

toc: true
toc_sticky: true
 
date: 2024-02-22
last_modified_at: 2024-02-22
---

# 1.개요
여기서는 빅투 환경에서 학습을 진행하기 앞서 앞으로 해결할 빅투의 학습 시 어려움에 대해서 알아보자.

# 2.행동의 다양성
빅투에서 패를 받았을 때 플러시의 경우의 수는 ${13 \choose 5} = 1287$이다. 이렇게 많은 족보의 경우의 수에 대하여 모든 행동을 정의하기에는 무리가 있어보인다. 그렇다면, 빅투에서의 action은 어떤 방식으로 다룰 수 있을까?

이를 다룬 포스트는 아래를 참고하자.

[Action state map](https://github.com/jh2525/ACM)

# 3.보상의 설정 문제
우리의 목표는 최종 칩 갯수에서의 **순위보상**을 최대화 하는 것이다. 그렇지만, 곧바로 이를 보상으로 설정하기엔 **희소 보상 문제**에 직결하게 된다. 또한, 어떤 한 플레이어의 칩의 갯수가 다 떨어지기 위해서는 많은 행동을 반복해야한다. 칩의 이동 개수도 불안정한데, 한 번의 게임당 칩의 변화는 최소 3개에서, 최대 $2^4\times(14 + 13\times2)  = 640$개 까지 될 수도 있다. 대부분의 게임은 칩의 개수가 10개 정도로 변할 것이라고 생각되지만, 칩의 개수의 변화는 매우 다양하기 때문에, 이 역시 학습 불안정성을 초래할 것이다.

이를 다룬 포스트는 아래를 참고하자.

[Global reward prediction]( : )